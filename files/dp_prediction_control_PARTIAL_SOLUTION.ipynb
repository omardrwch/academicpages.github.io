{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Creating an Environment\n",
    "\n",
    "Here, we will define an environment (a Markov Decision Process) with a finite number of states and actions. \n",
    "\n",
    "The states are defined by their indexes: S = 0, 1, ..., Ns\n",
    "\n",
    "And the same for the actions:            A = 0, 1, ..., Na\n",
    "\n",
    "The file `finite_env.py` contains an abstract class `FiniteEnv` which is used to implement the main features of an environment:\n",
    "\n",
    "* The transition probabilites, represented by an array `P` of shape (Ns, Na, Ns) such that `P[s, a, s']` = Prob($S_{t+1}=s'| S_t = s, A_t = a$);\n",
    "* A list of all possible states `env.states`;\n",
    "* An array containing the actions available in each state `env.action_sets`;\n",
    "* The current state of the environment `env.state`;\n",
    "* The discount factor `gamma`; \n",
    "* A function `reset` that puts an environment in a default state and returns this state (example: `initial_state = env.reset()`);\n",
    "* A function `step` that takes a step in the environment, it takes as input an action and returns the next state, the reward, a flag that indicates whether we reached a terminal state, and a dictionary with extra information.\n",
    "\n",
    "Read carefully the definition of the class `FiniteEnv` in `finite_env.py`. \n",
    "\n",
    "**Remark**: Here we put the discount factor `gamma` as an attribute of the environment, for convenience. However, we can also consider it an attribute of the agent instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from finite_env import FiniteEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a class that implements `FiniteEnv`. We need to implement all the abstract methods, define the state/action sets, the transition probabibities and the discount factor. \n",
    "\n",
    "The class `ToyEnv1` below does that, and represents a MDP with 3 states and 2 actions per state. A reward of 1 is obtained during the transition to the last state, and is 0 in all other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyEnv1(FiniteEnv):\n",
    "    \"\"\"\n",
    "    Enviroment with 3 states and 2 actions per state that gives a reward of 1 when going to the\n",
    "    last state and 0 otherwise.\n",
    "\n",
    "    Args:\n",
    "        gamma (float): discount factor\n",
    "        seed    (int): Random number generator seed\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=0.99, seed=42):\n",
    "        # Set seed\n",
    "        self.RS = np.random.RandomState(seed)\n",
    "\n",
    "        # Transition probabilities\n",
    "        # shape (Ns, Na, Ns)\n",
    "        # P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a)\n",
    "\n",
    "        Ns = 3\n",
    "        Na = 2\n",
    "        P = np.zeros((Ns, Na, Ns))\n",
    "\n",
    "        P[:, 0, :] = np.array([[0.25, 0.5, 0.25], [0.1, 0.7, 0.2], [0.1, 0.8, 0.1]])\n",
    "        P[:, 1, :] = np.array([[0.3, 0.3, 0.4], [0.7, 0.2, 0.1], [0.25, 0.25, 0.5]])\n",
    "\n",
    "        # Initialize base class\n",
    "        states = np.arange(Ns).tolist()\n",
    "        action_sets = [np.arange(Na).tolist()]*Ns\n",
    "        super().__init__(states, action_sets, P, gamma)\n",
    "\n",
    "    def reward_func(self, state, action, next_state):\n",
    "        return 1.0 * (next_state == self.Ns - 1)\n",
    "\n",
    "    def reset(self, s=0):\n",
    "        self.state = s\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.sample_transition(self.state, action)\n",
    "        reward = self.reward_func(self.state, action, next_state)\n",
    "        done = False\n",
    "        info = {}\n",
    "        self.state = next_state\n",
    "\n",
    "        observation = next_state\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def sample_transition(self, s, a):\n",
    "        prob = self.P[s,a,:]\n",
    "        s_ = self.RS.choice(self.states, p = prob)\n",
    "        return s_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how the environment works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToyEnv1 has 3 states and 2 actions\n",
      "The array representing the transition probabilities has shape  (3, 2, 3)\n",
      "\n",
      " Transition kernel for a = 0 \n",
      " [[0.25 0.5  0.25]\n",
      " [0.1  0.7  0.2 ]\n",
      " [0.1  0.8  0.1 ]]\n",
      "\n",
      " Transition kernel for a = 1 \n",
      " [[0.3  0.3  0.4 ]\n",
      " [0.7  0.2  0.1 ]\n",
      " [0.25 0.25 0.5 ]]\n"
     ]
    }
   ],
   "source": [
    "# Create ToyEnv1 object\n",
    "env = ToyEnv1(gamma=0.95)\n",
    "\n",
    "# Print some properties\n",
    "print(\"ToyEnv1 has %d states and %d actions\"%(env.Ns, env.Na))\n",
    "print(\"The array representing the transition probabilities has shape \", env.P.shape)\n",
    "# When an action is fixed, we obtain the transition kernel of a Markov chain:\n",
    "print(\"\\n Transition kernel for a = 0 \\n\", env.P[:, 0, :])\n",
    "print(\"\\n Transition kernel for a = 1 \\n\", env.P[:, 1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state =  0\n",
      "Action = 0, Next state = 1, Reward = 0.00\n"
     ]
    }
   ],
   "source": [
    "# Get initial state\n",
    "state = env.reset()\n",
    "print(\"Initial state = \", state)\n",
    "\n",
    "# Choose an action\n",
    "a = 0\n",
    "\n",
    "# Sample a transition\n",
    "next_state, reward, done, info = env.step(a)\n",
    "assert next_state==env.state # check that environment has been indeed updated\n",
    "print(\"Action = %d, Next state = %d, Reward = %0.2f\"%(a, next_state, reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Bellman operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a policy \n",
    "A deterministic Markov policy can be represented by a 1d array such that `policy[state]=action` represents the action to be taken in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random policy =  [0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# A random policy\n",
    "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
    "print(\"random policy = \", policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Bellman operator and greedy policies\n",
    "1. Write a function that takes an environment, a policy and a value function $V$ as input and returns the Bellman operator applied to $V$, $T^\\pi V$.\n",
    "2. Write a function that takes an environment and a value function $V$ as input and returns the Bellman optimality operator applied to $V$, $T^* V$ and the greedy policy with respect to $V$.\n",
    "3. Let $V_1$ and $V_2$ be value functions. Verify the contraction property: $||T^\\pi V_1 - T^\\pi V_2|| \\leq \\gamma ||V_1 - V_2||$ and $||T^* V_1 - T^* V_2|| \\leq \\gamma ||V_1 - V_2||$, where $||V|| = \\max_s |V(s)|$.\n",
    "4. Verify that: if $V_1 \\leq V_2$, then $T^\\pi V_1 \\leq T^\\pi V_2$ and $T^* V_1 \\leq T^* V_2$.\n",
    "5. Verify that: $T^\\pi V \\leq T^* V$\n",
    "\n",
    "\n",
    "### Reminder\n",
    "\n",
    "$$\n",
    "T^\\pi V(s) = \\sum_{s'}P(s'|s,\\pi(s))[ r(s, \\pi(s), s') + \\gamma V(s')] \\quad \\mbox{ for a deterministic policy } \\pi \n",
    "$$\n",
    "\n",
    "$$\n",
    "T^* V(s) = \\max_a \\sum_{s'}P(s'|s,a)[ r(s, a, s') + \\gamma V(s')]  \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Solution to 1.\n",
    "# --------------\n",
    "def bellman_operator(V, policy, env):\n",
    "    \"\"\"\n",
    "    Bellman operator. To be done!\n",
    "    \"\"\"\n",
    "    TV = np.zeros(env.Ns)\n",
    "\n",
    "    for s in env.states:\n",
    "        a = policy[s]\n",
    "        assert a in env.available_actions(s) # make sure this action is available\n",
    "        prob = env.P[s, a, :]\n",
    "        rewards = np.array([env.reward_func(s,a, s_) for s_ in env.states])\n",
    "        TV[s] = np.sum( prob*(rewards + env.gamma*V)  )\n",
    "        \n",
    "    return TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Solution to 2.\n",
    "# --------------\n",
    "def bellman_opt_operator(V, env):\n",
    "    \"\"\"\n",
    "    Bellman optimality operator. To be done!\n",
    "    \"\"\"\n",
    "    Q = -np.inf*np.ones((env.Ns, env.Na))\n",
    "    greedy_policy = np.zeros(env.Ns)\n",
    "    for s in env.states:\n",
    "        for a in env.available_actions(s):\n",
    "            prob = env.P[s, a, :]\n",
    "            rewards = np.array([float(env.reward_func(s,a, s_)) for s_ in env.states])\n",
    "            Q[s,a] = np.sum( prob*(rewards + env.gamma*V)  )\n",
    "\n",
    "    TV = np.max(Q, axis = 1)\n",
    "    argmax = np.argmax(Q, axis = 1)\n",
    "    greedy_policy = argmax\n",
    "    \n",
    "    return TV, greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contraction of Bellman operator: \n",
      "0.39128243097740323\n",
      "0.45359051706104503\n",
      "0.4981154400181063\n",
      "0.5701564639131153\n",
      "0.2690373847817199\n",
      "\n",
      " Contraction of Bellman optimality operator: \n",
      "0.683260900422996\n",
      "0.018013580143773047\n",
      "0.37461943626749716\n",
      "0.604173184855166\n",
      "0.6257444428610531\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "# Solution to 3.\n",
    "# --------------\n",
    "n_simulations = 5\n",
    "\n",
    "print(\"Contraction of Bellman operator: \")\n",
    "for ii in range(n_simulations):\n",
    "    V1 = np.random.randn(env.Ns)\n",
    "    V2 = np.random.randn(env.Ns)\n",
    "    policy = np.random.randint(env.Na, size = (env.Ns,))\n",
    "\n",
    "    # Contraction of Bellman operator\n",
    "    contraction = np.abs(bellman_operator(V1, policy, env) - bellman_operator(V2, policy, env)).max() / np.abs(V1-V2).max()\n",
    "    print(contraction)\n",
    "    assert contraction <= env.gamma\n",
    "\n",
    "print(\"\\n Contraction of Bellman optimality operator: \")\n",
    "for ii in range(n_simulations):\n",
    "    V1 = np.random.randn(env.Ns)\n",
    "    V2 = np.random.randn(env.Ns)\n",
    "\n",
    "    # Contraction of Bellman operator\n",
    "    contraction_opt = np.abs(bellman_opt_operator(V1, env)[0] - bellman_opt_operator(V2, env)[0]).max() / np.abs(V1-V2).max()\n",
    "    print(contraction_opt)\n",
    "    assert contraction_opt <= env.gamma\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff of Bellman operator: \n",
      "[-0.39799087 -0.26532724 -1.06130898]\n",
      "[-0.83973658 -1.06009967 -0.47763406]\n",
      "[0. 0. 0.]\n",
      "[-0.13969527 -0.21406868 -0.14184157]\n",
      "[0. 0. 0.]\n",
      "\n",
      " Diff of Bellman optimality operator: \n",
      "[-0.32476496 -0.45873184 -0.33044808]\n",
      "[-1.09552885 -1.2590533  -1.58226833]\n",
      "[-0.06974172 -0.02324724 -0.0581181 ]\n",
      "[-0.19422855 -0.27191998 -0.31076569]\n",
      "[-0.21883263 -0.08753305 -0.08753305]\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "# Solution to 4.\n",
    "# --------------\n",
    "\n",
    "print(\"Diff of Bellman operator: \")\n",
    "for ii in range(n_simulations):\n",
    "    V1 = np.random.randn(env.Ns)\n",
    "    V2 = V1 + np.random.randn(env.Ns).clip(min=0)\n",
    "    policy = np.random.randint(env.Na, size = (env.Ns,))\n",
    "    \n",
    "    # Diff of Bellman operator\n",
    "    diff = bellman_operator(V1, policy, env) - bellman_operator(V2, policy, env)\n",
    "    print(diff)\n",
    "    assert (diff > 0).sum() == 0\n",
    "\n",
    "print(\"\\n Diff of Bellman optimality operator: \")\n",
    "for ii in range(n_simulations):\n",
    "    V1 = np.random.randn(env.Ns)\n",
    "    V2 = V1 + np.random.randn(env.Ns).clip(min=0)\n",
    "\n",
    "    # Diff of Bellman operator\n",
    "    diff_opt = bellman_opt_operator(V1, env)[0] - bellman_opt_operator(V2, env)[0]\n",
    "    print(diff_opt)\n",
    "    assert (diff_opt > 0).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff between Bellman operator and Bellman optimality operator: \n",
      "[0. 0. 0.]\n",
      "[-0.39436934 -0.25200253 -1.06520198]\n",
      "[-0.27041804 -0.44143221  0.        ]\n",
      "[ 0.         -0.91058268  0.        ]\n",
      "[-0.32449739  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "# Solution to 5.\n",
    "# --------------\n",
    "\n",
    "print(\"Diff between Bellman operator and Bellman optimality operator: \")\n",
    "for ii in range(n_simulations):\n",
    "    V1 = np.random.randn(env.Ns)\n",
    "    policy = np.random.randint(env.Na, size = (env.Ns,))\n",
    "    \n",
    "    # Diff between Bellman operator and Bellman optimality operator\n",
    "    diff = bellman_operator(V1, policy, env) - bellman_opt_operator(V1, env)[0]\n",
    "    print(diff)\n",
    "    assert (diff > 0).sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Dynamic Programming\n",
    "\n",
    "When the transition probabilities and reward functions are known, we can use dynamic programming algorithms to find an optimal policy or the optimal value function, that is, \"solve\" the MDP. \n",
    "\n",
    "In this part, you will implement Value Iteration and Policy Iteration to solve an MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Value Iteration \n",
    "\n",
    "1. (Policy evaluation) Write a function that takes as input an initial value function `V0`, a policy `pi` and an environment `env` and returns a vector `V` such that $||T^\\pi V -  V ||_\\infty \\leq \\varepsilon $.\n",
    "2. (Optimal Value function) Write a function that takes as input an initial value function `V0` and an environment `env` and returns a vector `V` such that $||T^* V -  V ||_\\infty \\leq \\varepsilon $ and the greedy policy with respect to $V$.\n",
    "3. Test the convergence of the functions you implemented.\n",
    "\n",
    "\n",
    "Reminder:\n",
    "\n",
    "* For any $V_0$, let $V_n = T^\\pi V_{n-1}$. We have $\\lim_{n\\to\\infty}V_n = V^\\pi$ and $V^\\pi = T^\\pi V^\\pi$\n",
    "* For any $V_0$, let $V_n = T^* V_{n-1}$. We have $\\lim_{n\\to\\infty}V_n = V^*$ and $V^* = T^* V^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Solution to 1.\n",
    "# --------------\n",
    "def value_iteration(V0, pi, env, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Finding the value of a policy. To be done!\n",
    "    \"\"\"\n",
    "    it = 1\n",
    "    V = V0\n",
    "    while True:\n",
    "        TV = bellman_operator(V, pi, env)        \n",
    "        err = np.abs(TV-V).max() \n",
    "        assert np.sum((TV - V) < 0.0) == 0.0, \"V must increase!\"\n",
    "\n",
    "        if err < epsilon:\n",
    "            return TV \n",
    "\n",
    "        V = TV\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Solution to 2.\n",
    "# --------------\n",
    "def opt_value_iteration(V0, env, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Finding the optimal value function. To be done!\n",
    "    \"\"\"\n",
    "    it = 1\n",
    "    V = V0\n",
    "    while True:\n",
    "        TV, greedy_policy = bellman_opt_operator(V, env)\n",
    "\n",
    "        err = np.abs(TV-V).max() \n",
    "        if err < epsilon:\n",
    "            return TV, greedy_policy\n",
    "\n",
    "        V = TV\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm of T_pi(V) - V =  9.214146272640278e-07\n",
      "norm of T^*(V) - V =  9.158036746725884e-07\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "# Solution to 3.\n",
    "# --------------\n",
    "epsilon = 1e-6\n",
    "V0 = np.zeros(env.Ns)\n",
    "pi = np.random.randint(env.Na, size = (env.Ns,))\n",
    "\n",
    "V_ = value_iteration(V0, pi, env, epsilon)\n",
    "err = np.abs(V_ - bellman_operator(V_, pi, env)).max()\n",
    "print(\"norm of T_pi(V) - V = \", err)\n",
    "assert err <= epsilon\n",
    "\n",
    "\n",
    "V_, greedy_policy = opt_value_iteration(V0, env, epsilon)\n",
    "err = np.abs(V_ - bellman_opt_operator(V_, env)[0]).max()\n",
    "print(\"norm of T^*(V) - V = \", err)\n",
    "assert err <= epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Exact Policy Evaluation\n",
    "\n",
    "The value function $V^\\pi$ of a policy $\\pi$ can also be solved exactly by a linear system: $(I - \\gamma P_\\pi) V^\\pi = r_\\pi $ where $P_\\pi$ is the transition kernel (matrix) induced by $\\pi$: $P_\\pi(s, s') = P(S_{t+1}=s' | S_t = s, A_t = \\pi(s)) $ and $r_\\pi(s') = \\sum_{s} r(s,\\pi(s),s') P(S_{t+1}=s' | S_t = s, A_t = \\pi(s)) $ is the average reward obtained in state $s'$ by following the policy $\\pi$.  \n",
    "\n",
    "1. Write a function `exact_policy_eval` that takes as input an environment `env` and a policy `pi` and returns the exact value of $V^\\pi$.\n",
    "2. Let:\n",
    "\n",
    "    `V_opt, greedy_policy = opt_value_iteration(V0, env)`\n",
    "    and \n",
    "    \n",
    "    `V_pol = exact_policy_eval(greedy_policy, env)`\n",
    "    \n",
    "    Verify that `V_opt` and `V_pol` are close enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Solution to 1.\n",
    "# --------------\n",
    "def exact_policy_eval(pi, env):\n",
    "    \"\"\"\n",
    "    To be done!\n",
    "    \"\"\"\n",
    "    # Compute the transition matrix P_pi and reward vector r_pi\n",
    "    P_pi = np.zeros((env.Ns, env.Ns))\n",
    "    r_pi = np.zeros(env.Ns)\n",
    "    \n",
    "    for s in env.states:\n",
    "        for s_ in env.states:\n",
    "            a = pi[s]\n",
    "            P_pi[s, s_] = env.P[s,a,s_]\n",
    "            r_pi[s] += env.reward_func(s,a,s_)*env.P[s,a,s_]\n",
    "\n",
    "    A = np.eye(env.Ns) - env.gamma*P_pi\n",
    "    b = r_pi\n",
    "\n",
    "    # Solve linear system\n",
    "    V = np.linalg.solve(A, b)\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference =  9.158036746725884e-07\n",
      "V_opt =  [7.21271907 6.90579408 7.33932563]\n",
      "V_pol =  [7.21273739 6.90581239 7.33934395]\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "# Solution to 2.\n",
    "# --------------\n",
    "epsilon = 1e-6\n",
    "V0 = np.zeros(env.Ns)\n",
    "pi = np.random.randint(env.Na, size = (env.Ns,))\n",
    "\n",
    "V_opt, greedy_policy = opt_value_iteration(V0, env, epsilon)\n",
    "V_pol = exact_policy_eval(greedy_policy, env)\n",
    "\n",
    "\n",
    "err = np.abs(V_ - bellman_opt_operator(V_, env)[0]).max()\n",
    "print(\"difference = \", err)\n",
    "print(\"V_opt = \", V_opt)\n",
    "print(\"V_pol = \", V_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Policy Iteration\n",
    "\n",
    "1. Write a function `policy_iteration` that takes an initial policy `pi0` and an environment `env` as input and returns the optimal policy.\n",
    "2. Let:\n",
    "\n",
    "    `V_opt, greedy_policy = opt_value_iteration(V0, env)`\n",
    "    and \n",
    "    \n",
    "    `pi = policy_iteration(pi0, env)`\n",
    "    \n",
    "    Verify that `greedy_policy` and `pi` are equal. \n",
    "    \n",
    "Reminder:\n",
    "\n",
    "* For any $\\pi_0$, let $\\pi_k$ be the greedy policy with respect to $V^{\\pi_{k-1}}$, that is:\n",
    "\n",
    "$$\n",
    "  \\pi_k(s) \\in \\arg\\max_a \\sum_{s'}P(s'|s,a)[ r(s, a, s') + \\gamma V^{\\pi_{k-1}}(s')]\n",
    "$$\n",
    "\n",
    "Then, there exists a value $n^* < \\infty$ such that $\\pi_{n^*}$ is an optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Solution to 1.\n",
    "# --------------\n",
    "def policy_iteration(pi0, env):\n",
    "    \"\"\"\n",
    "    To be done!\n",
    "    \"\"\"\n",
    "    it = 1\n",
    "    policy = pi0\n",
    "    while True:\n",
    "        # Policy evaluation\n",
    "        V = exact_policy_eval(policy, env)\n",
    "\n",
    "        # Policy improvement\n",
    "        TV, greedy_policy = bellman_opt_operator(V, env)\n",
    "        if (np.abs(TV-V).max() < 1e-12):\n",
    "            return greedy_policy\n",
    "\n",
    "        policy = greedy_policy\n",
    "        it += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n",
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "# Solution to 2.\n",
    "# --------------\n",
    "V0 = np.zeros(env.Ns)\n",
    "pi0 = np.random.randint(env.Na, size = (env.Ns,))\n",
    "\n",
    "V_opt, greedy_policy = opt_value_iteration(V0, env)\n",
    "pi = policy_iteration(pi0, env)\n",
    "\n",
    "print(pi)\n",
    "print(greedy_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV. Value Prediction Algorithms\n",
    "\n",
    "When the transition dynamics and the reward function are *not* available, we cannot use dynamic programming algorithms to evaluate a policy or find a near-optimal policy. In this case, we must rely on samples of trajectories to estimate them.\n",
    "\n",
    "In this part, you will implement algorithms to estimate the value of a policy using samples collected by an agent interacting with the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampling a trajectory \n",
    "\n",
    "The algorithms in this part require the agent to interact with the environment to sample trajectories. The code below illustrates how to sample one trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 2, 1.0)\n",
      "(2, 1, 1, 0.0)\n",
      "(1, 0, 1, 0.0)\n",
      "(1, 0, 1, 0.0)\n",
      "(1, 0, 1, 0.0)\n",
      "(1, 0, 0, 0.0)\n",
      "(0, 1, 2, 1.0)\n",
      "(2, 1, 1, 0.0)\n",
      "(1, 0, 1, 0.0)\n",
      "(1, 0, 0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# Sampling a trajectory given a policy\n",
    "policy = [1, 0, 1]\n",
    "state = env.reset() # initial state\n",
    "T = 10  # length of the trajectory\n",
    "\n",
    "trajectory = []\n",
    "for t in range(T):\n",
    "    action = policy[state]\n",
    "    next_state, reward, done, info = env.step(a)\n",
    "    if done:\n",
    "        break # stop if we reach a terminal state\n",
    "    trajectory += [(state, action, next_state, reward)]\n",
    "    print(trajectory[t])\n",
    "    state = next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.1: TD(0) \n",
    "\n",
    "Given a trajectory $ (x_t, x_{t+1}, r_t)_{t\\geq 0} $ , the $t$-th step of TD(0) performs the following calculations:\n",
    "\n",
    "$ \\delta_t = r_t + \\gamma \\hat{V}_t(x_{t+1}) - \\hat{V}_t(x_t)$\n",
    "\n",
    "$ \\hat{V}_{t+1}(x) = \\hat{V}_t(x) + \\alpha_t(x)\\delta_t\\mathbb{1}\\{x=x_t\\}  $ \n",
    "\n",
    "where $\\alpha_t(x_t)$ is the step size and $\\delta_t$ is called *temporal difference*.\n",
    "\n",
    "Write a function `td_zero` that takes as input an environment `env`, a time horizon `T`, a policy `pi` and returns $\\hat{V}_T$ and `V_it`, a 2d array such that `V_it[t, :]` is equal to $\\hat{V}_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_zero(env, pi, T):\n",
    "    \"\"\"\n",
    "    TD(0) algorithm. To be done!\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.Ns)\n",
    "    V_it = np.zeros((env.Ns, T))\n",
    "    return V, V_it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.2: Every-visit Monte-Carlo\n",
    "\n",
    "Given a trajectory $ (x_t, x_{t+1}, r_t)_{t\\geq 0} $ and a time horizon $T$ , the $t$-th step of every-visit MC performs the following calculations:\n",
    "\n",
    "$ R_t = \\sum_{s=t}^T \\gamma^{s-t}r_s $\n",
    "\n",
    "$ \\delta_t = R_t - \\hat{V}_t(x_t)$\n",
    "\n",
    "$ \\hat{V}_{t+1}(x) = \\hat{V}_t(x) + \\alpha_t(x)\\delta_t \\mathbb{1}\\{x=x_t\\}  $ \n",
    "\n",
    "Write a function `every_visit_mc` that takes as input an environment `env`, a time horizon `T`, a policy `pi` and returns $\\hat{V}_T$ and `V_it`, a 2d array such that `V_it[t, :]` is equal to $\\hat{V}_t$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_visit_mc(env, pi, T):\n",
    "    \"\"\"\n",
    "    Every-visit Monte-Carlo. To be done!\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.Ns)\n",
    "    V_it = np.zeros((env.Ns, T))\n",
    "    return V, V_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.3 TD($\\lambda$)\n",
    "\n",
    "Given a trajectory $ (x_t, x_{t+1}, r_t)_{t\\geq 0} $ and, the $t$-th step of TD($\\lambda$) performs the following calculations:\n",
    "\n",
    "$ \\delta_t = r_t + \\gamma \\hat{V}_t(x_{t+1}) - \\hat{V}_t(x_t)$\n",
    "\n",
    "$ z_{t+1}(x) = \\mathbb{1}\\{x=x_t\\} + \\gamma \\lambda z_t(x)  $ \n",
    "\n",
    "$ \\hat{V}_{t+1}(x) = \\hat{V}_t(x) + \\alpha_t(x)\\delta_t z_{t+1}(x)  $ \n",
    "\n",
    "$ z_0(x) = 0 $\n",
    "\n",
    "for all states $x$.\n",
    " \n",
    "Write a function `td_lambda` that takes as input an environment `env`, a time horizon `T`, a policy `pi` and returns $\\hat{V}_T$ and `V_it`, a 2d array such that `V_it[t, :]` is equal to $\\hat{V}_t$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_lambda(env, pi, l, T):\n",
    "    \"\"\"\n",
    "    TD(lambda) algorithm. To be done!\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.Ns)\n",
    "    V_it = np.zeros((env.Ns, T))\n",
    "    z = np.zeros(env.Ns)        \n",
    "    return V, V_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.4: Comparing the algorithms \n",
    "\n",
    "Using the functions implemented above, compare the convergence of TD(0), every-visit Monte-Carlo and TD(lambda) for different values of lambda. You can use the dynamic programming functions that you implemented before to compare the true value function and its estimate $\\hat{V}_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# 5.4\n",
    "# ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.5: Non-ergodic MDPs \n",
    "\n",
    "The value function $\\hat{V}(x)$ estimated by the algorithms above converges to the true value function under some conditions on the step size and provided that all states are visited infinitely often, which is not always the case in a single trajectory. In an ergodic MDP (i.e., the Markov chain induced by any policy is ergodic), a single trajectory is enough. How would you modify the functions above to ensure the convergence to the true value function? (no need to implement the modifications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# 5.5\n",
    "# ---------------\n",
    "\n",
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. Control Algorithms\n",
    "\n",
    "In Part IV, you implemented algorithms to estimate the value of a policy. Now, you will implement algorithms to estimate the optimal value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.1: Q-Learning\n",
    "\n",
    "Q-learning is an algorithm to estimate the Q function, which represents the value of each state-action pair:\n",
    "\n",
    "$$\n",
    "Q(s,a) = r(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_a Q(s',a)\n",
    "$$\n",
    "\n",
    "If we have access to the Q function, an optimal policy can be computed as:\n",
    "\n",
    "$$ \\pi^*(s) \\in \\arg\\max_a Q(s,a) $$\n",
    "\n",
    "The Q-Learning algorithm allows us to estimate the optimal Q function using only trajectories from the MDP obtained by following a $\\varepsilon$-greedy policy. The Q-learning update at time $t$ is done as follows:\n",
    "\n",
    "1. In state $s_t$, take action $a_t$  such that $a_t$ is random with probability $\\varepsilon$ and $a_t \\in \\arg\\max_a \\hat{Q}_t(s_t,a) $ with probability $1-\\varepsilon$;\n",
    "2. Observe $s_{t+1}$ and reward $r_t$;\n",
    "3. Compute $\\delta_t = r_t + \\gamma \\max_a \\hat{Q}_t(s_{t+1}, a) - \\hat{Q}_t(s_t, a_t)$;\n",
    "4. Update $\\hat{Q}_{t+1}(s, a) = \\hat{Q}_t(s, a) + \\alpha_t(s,a)\\delta_t\\mathbb{1}\\{s=s_t, a=a_t\\}  $\n",
    "\n",
    "\n",
    "Implement the Q-Learning algorithm and verify that it converges to the optimal value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# 6.1 (implementing Q-Learning)\n",
    "# ---------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# 6.1 (check the convergence of Q-Learning)\n",
    "# ---------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.2: SARSA\n",
    "\n",
    "We can also define the Q function for a given policy $\\pi$ (possibly stochastic):\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = r(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\sum_{a'}Q^\\pi(s',a') \\pi(a'|s')\n",
    "$$\n",
    "\n",
    "SARSA is similar to Q-learning, but instead of following a $\\varepsilon$-greedy policy, it follows an exploratory (stochastic) policy $\\pi_Q$ and estimates the value of this policy (it is an *on-policy* algorithm). One possible choice is:\n",
    "\n",
    "$$\n",
    "\\pi_Q(a|s) = \\frac{ \\exp(\\tau^{-1}Q(s,a))  }{\\sum_{a'}\\exp(\\tau^{-1}Q(s,a')) }\n",
    "$$\n",
    "where $\\tau$ is a \"temperature\" parameter: when $\\tau$ approaches 0, $\\pi_Q(a|s)$ approaches the greedy (deterministic) policy $a \\in \\arg\\max_{a'}Q(s,a')$.\n",
    "\n",
    "At each time $t$, SARSA keeps an estimate $\\hat{Q}_t$ of the true Q function and uses $\\pi_{\\hat{Q}_t}(a|s)$ to choose the action $a_t$. If $\\tau \\to 0$ with a proper rate as $t \\to \\infty$, $\\hat{Q}_t$ converges to $Q$ and $\\pi_{\\hat{Q}_t}(a|s)$ converges to the optimal policy $\\pi^*$. \n",
    "\n",
    "The SARSA update at time $t$ is done as follows:\n",
    "\n",
    "1. In state $s_t$, take action $a_t$ sampled at the previous step using $\\pi_{\\hat{Q}_{t-1}}(a|s_t)$ ;\n",
    "2. Observe $s_{t+1}$ and reward $r_t$;\n",
    "3. Sample the next action $a_{t+1} \\sim \\pi_{\\hat{Q}_t}(a|s_{t+1})$;\n",
    "4. Compute $\\delta_t = r_t + \\gamma \\hat{Q}_t(s_{t+1}, a_{t+1}) - \\hat{Q}_t(s_t, a_t)$\n",
    "5. Update $\\hat{Q}_{t+1}(s, a) = \\hat{Q}_t(s, a) + \\alpha_t(s,a)\\delta_t\\mathbb{1}\\{s=s_t, a=a_t\\}$\n",
    "\n",
    "\n",
    "Implement the SARSA algorithm and verify that it converges to the optimal value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# 6.2 (implementation of SARSA)\n",
    "# ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# 6.2 (convergence of SARSA)\n",
    "# ---------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
